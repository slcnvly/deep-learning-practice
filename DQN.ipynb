{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMlDttisIi2ZTcptTeVZTiK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slcnvly/deep-learning-practice/blob/master/DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "!pip install gym pyvirtualdisplay imageio imageio-ffmpeg > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "IxlmsB5VtSo6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7bOexc4vXKtP"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import collections\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import imageio\n",
        "from pyvirtualdisplay import Display"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "def save_dqn_gif(model, env_name='CartPole-v1', filename=\"cartpole_result.gif\"):\n",
        "    import torch  # 혹시 몰라 import 추가\n",
        "\n",
        "    # 모델이 지금 GPU에 있는지 CPU에 있는지 스스로 확인하게 함\n",
        "    # 사용자가 device를 뭘로 설정했든 상관없이 모델이 있는 곳으로 따라감\n",
        "    target_device = next(model.parameters()).device\n",
        "\n",
        "    model.eval() # 평가 모드\n",
        "\n",
        "    env = gym.make(env_name, render_mode='rgb_array')\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    frames = []\n",
        "    step_count = 0\n",
        "    max_steps = 500\n",
        "\n",
        "    print(f\"[{filename}] GIF 생성 시작... (모델 위치: {target_device})\")\n",
        "\n",
        "    while not done and step_count < max_steps:\n",
        "        frames.append(env.render())\n",
        "\n",
        "        # 입력 데이터를 모델이 있는 곳(target_device)으로 강제 이동\n",
        "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(target_device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_values = model(state_tensor)\n",
        "\n",
        "        action = q_values.argmax().item()\n",
        "\n",
        "        next_state, reward, done, truncated, _ = env.step(action)\n",
        "        state = next_state\n",
        "        step_count += 1\n",
        "\n",
        "    env.close()\n",
        "    imageio.mimsave(filename, frames, fps=30)\n",
        "    print(f\"✅ 저장 완료! {filename}을 다운로드 받으세요.\")"
      ],
      "metadata": {
        "id": "diYUoA20tsf7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 설정\n",
        "learning_rate = 0.0005\n",
        "gamma         = 0.98\n",
        "buffer_limit  = 50000\n",
        "batch_size    = 32\n",
        "\n",
        "# 1. Experience Replay Buffer\n",
        "class ReplayBuffer():\n",
        "    def __init__(self):\n",
        "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
        "\n",
        "    def put(self, transition):\n",
        "        S, A, R, S_prime, Done = transition\n",
        "        self.buffer.append((S, A, R, S_prime, Done))\n",
        "\n",
        "    def sample(self, n):\n",
        "        # 버퍼에서 n개의 데이터를 무작위로 추출하여 s, a, r, s_prime, done_mask 형태의 텐서로 반환하는 로직\n",
        "        mini_batch = random.sample(self.buffer, n)\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, done_lst = [], [], [], [], []\n",
        "        for transition in mini_batch:\n",
        "            s, a, r, s_prime, done = transition\n",
        "            s_lst.append(s) # s는 이미 여러개의 값(ex. 위치, 속도, 각도, 각속도)을 가진 리스트\n",
        "            a_lst.append([a])\n",
        "            r_lst.append([r])\n",
        "            s_prime_lst.append(s_prime)\n",
        "            done_mask = 0.0 if done else 1.0\n",
        "            done_lst.append([done_mask]) # 안전을 위해 그냥 형태 통일\n",
        "\n",
        "        return torch.tensor(np.array(s_lst), dtype=torch.float), \\\n",
        "           torch.tensor(a_lst), \\\n",
        "           torch.tensor(np.array(r_lst), dtype=torch.float), \\\n",
        "           torch.tensor(np.array(s_prime_lst), dtype=torch.float), \\\n",
        "           torch.tensor(np.array(done_lst), dtype=torch.float)\n",
        "        # 리스트->numpy->tensor의 변환 이유는 numpy 배열의 메모리 저장 방식(연속적)의 수혜를 받기 위해, 이후 딥러닝 라이브러리 호환성을 위해 tensor로 변환\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "xofozg4Afzzc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Q-네트워크 (Q-Network)\n",
        "class Qnet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Qnet, self).__init__()\n",
        "        #신경망의 층(Linear Layer)을 정의\n",
        "        self.fc1 = nn.Linear(4, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def sample_action(self, obs, epsilon):\n",
        "        # epsilon-greedy 방식으로 행동을 선택\n",
        "        coin = random.random()\n",
        "        if coin < epsilon:\n",
        "          return random.randint(0,1)\n",
        "        else:\n",
        "          out = self.forward(obs)\n",
        "          return out.argmax().item() # 신경망의 결과는 각 행동들의 Q함수 --> 따라서 argmax를 이용해 행동 자체를 뽑아냄"
      ],
      "metadata": {
        "id": "0PmDOXE4f5xp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 학습 로직 (Train Function)\n",
        "def train(q, q_target, memory, optimizer):\n",
        "    # 1. 메모리에서 배치를 샘플링\n",
        "    S, A, R, S_prime, Done = memory.sample(batch_size)\n",
        "    # 2. 타겟 y_i 계산 (Target Network 사용) --> Moving target 문제를 해결하기 위해..\n",
        "    q_target_out = q_target(S_prime)\n",
        "    # 3. 현재 Q값 계산 (Q-Network 사용)\n",
        "    q_out = q(S)\n",
        "    # 4. Loss 계산 및 역전파(Backpropagation)\n",
        "    q_a = q_out.gather(1, A) # gather함수 --> 1-dim(열 방향)의 행동 A값 인덱스의 q값만 추출하는 코드\n",
        "    max_q_prime = q_target_out.max(1)[0].unsqueeze(1) # max(1)은 두가지 반환 --> [0]에서 최대값, [1]에서 argmax / unsqueeze를 통해 q_a와 차원을 맞춤([batchsize]1차원 --> [batchsize, 1]로)\n",
        "    target = R + gamma * max_q_prime * Done\n",
        "    loss = F.smooth_l1_loss(q_a, target) # L = 1/2 * (Target - current_Q)^2 의 식으로 계산\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward() # L의 gradients 계산\n",
        "    torch.nn.utils.clip_grad_norm_(q.parameters(), 1.0) # 안정장치 - norm이 1.0을 넘어가는 보상에 대해서 1.0으로 고\n",
        "    optimizer.step() # Q_net의 가중치 학습"
      ],
      "metadata": {
        "id": "6v_L0d5Pf9-d"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 메인 루프 (Main Loop)\n",
        "def main():\n",
        "    env = gym.make('CartPole-v1')\n",
        "    q = Qnet()\n",
        "    q_target = Qnet()\n",
        "    q_target.load_state_dict(q.state_dict()) # 가중치 동기화\n",
        "    memory = ReplayBuffer()\n",
        "\n",
        "    optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
        "    score = 0.0\n",
        "\n",
        "    for n_epi in range(10000):\n",
        "        epsilon = max(0.01, 0.3 - 0.3*(n_epi/2000)) # Linear annealing(선형 감쇠) --> 학습을 해 나갈수록 입실론 값을 줄여나감\n",
        "        s, _ = env.reset() # env.reset() --> 환경을 초기 상태로 reset + 첫번째 state을 반환\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # 에이전트가 행동을 선택하고 환경과 상호작용\n",
        "            action = q.sample_action(torch.from_numpy(s).float(), epsilon)\n",
        "            s_prime, r, done, _, _ = env.step(action)\n",
        "            memory.put((s, action, r/100.0, s_prime, done)) # 딥러닝은 보통 입출력이 -1~1 일때 최적화가 가장 잘 작동.. normalization을 통해 학습의 안정화를 꾀함\n",
        "            # 학습 조건이 맞으면 train(q, q_target, memory, optimizer) 호출\n",
        "            if memory.size() > 2000:\n",
        "                train(q, q_target, memory, optimizer)\n",
        "\n",
        "            s = s_prime\n",
        "            score += r\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # 일정 주기마다 Target Network를 업데이트(가중치 복사)\n",
        "\n",
        "        if n_epi % 20 == 0 and n_epi != 0:\n",
        "            # 학습 중 평균 점수 (탐험 때문에 낮을 수 있음)\n",
        "            avg_score = score / 20\n",
        "\n",
        "            # 잠깐 입실론 0으로 설정하고 1판 테스트 (진짜 실력 측정)\n",
        "            test_score = 0\n",
        "            s_test, _ = env.reset()\n",
        "            done_test = False\n",
        "            test_step_count = 0\n",
        "            max_test_steps = 500\n",
        "            while not done_test:\n",
        "                # 입실론 0.0 (무조건 Greedy)\n",
        "                a_test = q.sample_action(torch.from_numpy(s_test).float(), 0.0)\n",
        "                s_test, r_test, done_test, _, _ = env.step(a_test)\n",
        "                test_score += r_test\n",
        "                test_step_count += 1\n",
        "                # 무한루프 방지 - 너무 오래 버티면 강제로 끝내기\n",
        "                if test_step_count >= max_test_steps:\n",
        "                    break\n",
        "\n",
        "            # 결과 출력\n",
        "            print(f\"Episode: {n_epi} | Train Score: {avg_score:.1f} | Real Test: {test_score:.1f} | Eps: {epsilon:.2f}\")\n",
        "\n",
        "            # 모델 저장 및 타겟 업데이트\n",
        "            q_target.load_state_dict(q.state_dict())\n",
        "            if test_score >= 500.0:\n",
        "                print(f\"mission complete on Episode {n_epi}! terminate learning.\")\n",
        "                break\n",
        "            score = 0.0\n",
        "\n",
        "    env.close()\n",
        "    save_dqn_gif(q, filename=\"final_success.gif\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "ow0AgkGAf-MB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c34639d-c372-477e-eee7-4902eea37786"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 20 | Train Score: 61.0 | Real Test: 35.0 | Eps: 0.30\n",
            "Episode: 40 | Train Score: 41.2 | Real Test: 10.0 | Eps: 0.29\n",
            "Episode: 60 | Train Score: 10.8 | Real Test: 10.0 | Eps: 0.29\n",
            "Episode: 80 | Train Score: 11.8 | Real Test: 9.0 | Eps: 0.29\n",
            "Episode: 100 | Train Score: 12.2 | Real Test: 11.0 | Eps: 0.28\n",
            "Episode: 120 | Train Score: 12.9 | Real Test: 13.0 | Eps: 0.28\n",
            "Episode: 140 | Train Score: 20.6 | Real Test: 10.0 | Eps: 0.28\n",
            "Episode: 160 | Train Score: 61.1 | Real Test: 100.0 | Eps: 0.28\n",
            "Episode: 180 | Train Score: 58.0 | Real Test: 153.0 | Eps: 0.27\n",
            "Episode: 200 | Train Score: 203.8 | Real Test: 203.0 | Eps: 0.27\n",
            "Episode: 220 | Train Score: 150.3 | Real Test: 159.0 | Eps: 0.27\n",
            "Episode: 240 | Train Score: 137.7 | Real Test: 121.0 | Eps: 0.26\n",
            "Episode: 260 | Train Score: 117.3 | Real Test: 110.0 | Eps: 0.26\n",
            "Episode: 280 | Train Score: 109.8 | Real Test: 98.0 | Eps: 0.26\n",
            "Episode: 300 | Train Score: 99.5 | Real Test: 93.0 | Eps: 0.26\n",
            "Episode: 320 | Train Score: 123.8 | Real Test: 222.0 | Eps: 0.25\n",
            "Episode: 340 | Train Score: 63.6 | Real Test: 100.0 | Eps: 0.25\n",
            "Episode: 360 | Train Score: 109.9 | Real Test: 113.0 | Eps: 0.25\n",
            "Episode: 380 | Train Score: 121.8 | Real Test: 240.0 | Eps: 0.24\n",
            "Episode: 400 | Train Score: 124.8 | Real Test: 141.0 | Eps: 0.24\n",
            "Episode: 420 | Train Score: 140.9 | Real Test: 243.0 | Eps: 0.24\n",
            "Episode: 440 | Train Score: 112.6 | Real Test: 110.0 | Eps: 0.23\n",
            "Episode: 460 | Train Score: 136.7 | Real Test: 99.0 | Eps: 0.23\n",
            "Episode: 480 | Train Score: 134.2 | Real Test: 109.0 | Eps: 0.23\n",
            "Episode: 500 | Train Score: 96.9 | Real Test: 141.0 | Eps: 0.22\n",
            "Episode: 520 | Train Score: 101.5 | Real Test: 92.0 | Eps: 0.22\n",
            "Episode: 540 | Train Score: 135.9 | Real Test: 97.0 | Eps: 0.22\n",
            "Episode: 560 | Train Score: 75.2 | Real Test: 108.0 | Eps: 0.22\n",
            "Episode: 580 | Train Score: 80.8 | Real Test: 103.0 | Eps: 0.21\n",
            "Episode: 600 | Train Score: 73.7 | Real Test: 105.0 | Eps: 0.21\n",
            "Episode: 620 | Train Score: 64.4 | Real Test: 54.0 | Eps: 0.21\n",
            "Episode: 640 | Train Score: 93.1 | Real Test: 104.0 | Eps: 0.20\n",
            "Episode: 660 | Train Score: 79.8 | Real Test: 107.0 | Eps: 0.20\n",
            "Episode: 680 | Train Score: 85.3 | Real Test: 116.0 | Eps: 0.20\n",
            "Episode: 700 | Train Score: 62.0 | Real Test: 67.0 | Eps: 0.20\n",
            "Episode: 720 | Train Score: 62.7 | Real Test: 110.0 | Eps: 0.19\n",
            "Episode: 740 | Train Score: 72.9 | Real Test: 140.0 | Eps: 0.19\n",
            "Episode: 760 | Train Score: 50.6 | Real Test: 101.0 | Eps: 0.19\n",
            "Episode: 780 | Train Score: 58.5 | Real Test: 135.0 | Eps: 0.18\n",
            "Episode: 800 | Train Score: 57.0 | Real Test: 88.0 | Eps: 0.18\n",
            "Episode: 820 | Train Score: 66.0 | Real Test: 263.0 | Eps: 0.18\n",
            "Episode: 840 | Train Score: 68.1 | Real Test: 91.0 | Eps: 0.17\n",
            "Episode: 860 | Train Score: 57.5 | Real Test: 92.0 | Eps: 0.17\n",
            "Episode: 880 | Train Score: 57.0 | Real Test: 95.0 | Eps: 0.17\n",
            "Episode: 900 | Train Score: 85.8 | Real Test: 104.0 | Eps: 0.16\n",
            "Episode: 920 | Train Score: 66.8 | Real Test: 500.0 | Eps: 0.16\n",
            "mission complete on Episode 920! terminate learning.\n",
            "[final_success.gif] GIF 생성 시작... (모델 위치: cpu)\n",
            "✅ 저장 완료! final_success.gif을 다운로드 받으세요.\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4IL4V9OP6PbsxSkhnpTeg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slcnvly/deep-learning-practice/blob/master/DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "7bOexc4vXKtP"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import collections\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 설정\n",
        "learning_rate = 0.0005\n",
        "gamma         = 0.98\n",
        "buffer_limit  = 50000\n",
        "batch_size    = 32\n",
        "\n",
        "# 1. Experience Replay Buffer\n",
        "class ReplayBuffer():\n",
        "    def __init__(self):\n",
        "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
        "\n",
        "    def put(self, transition):\n",
        "        S, A, R, S_prime, Done = transition\n",
        "        self.buffer.append((S, A, R, S_prime, Done))\n",
        "\n",
        "    def sample(self, n):\n",
        "        # 버퍼에서 n개의 데이터를 무작위로 추출하여 s, a, r, s_prime, done_mask 형태의 텐서로 반환하는 로직\n",
        "        mini_batch = random.sample(self.buffer, n)\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, done_lst = [], [], [], [], []\n",
        "        for transition in mini_batch:\n",
        "            s, a, r, s_prime, done = transition\n",
        "            s_lst.append(s) # s는 이미 여러개의 값(ex. 위치, 속도, 각도, 각속도)을 가진 리스트\n",
        "            a_lst.append([a])\n",
        "            r_lst.append([r])\n",
        "            s_prime_lst.append(s_prime)\n",
        "            done_mask = 0.0 if done else 1.0\n",
        "            done_lst.append([done_mask]) # 안전을 위해 그냥 형태 통일\n",
        "\n",
        "        return torch.tensor(np.array(s_lst), dtype=torch.float), \\\n",
        "           torch.tensor(a_lst), \\\n",
        "           torch.tensor(np.array(r_lst), dtype=torch.float), \\\n",
        "           torch.tensor(np.array(s_prime_lst), dtype=torch.float), \\\n",
        "           torch.tensor(np.array(done_lst), dtype=torch.float)\n",
        "        # 리스트->numpy->tensor의 변환 이유는 numpy 배열의 메모리 저장 방식(연속적)의 수혜를 받기 위해, 이후 딥러닝 라이브러리 호환성을 위해 tensor로 변환\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "xofozg4Afzzc"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Q-네트워크 (Q-Network)\n",
        "class Qnet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Qnet, self).__init__()\n",
        "        #신경망의 층(Linear Layer)을 정의\n",
        "        self.fc1 = nn.Linear(4, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def sample_action(self, obs, epsilon):\n",
        "        # epsilon-greedy 방식으로 행동을 선택\n",
        "        coin = random.random()\n",
        "        if coin < epsilon:\n",
        "          return random.randint(0,1)\n",
        "        else:\n",
        "          out = self.forward(obs)\n",
        "          return out.argmax().item() # 신경망의 결과는 각 행동들의 Q함수 --> 따라서 argmax를 이용해 행동 자체를 뽑아냄"
      ],
      "metadata": {
        "id": "0PmDOXE4f5xp"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 학습 로직 (Train Function)\n",
        "def train(q, q_target, memory, optimizer):\n",
        "    # 1. 메모리에서 배치를 샘플링\n",
        "    S, A, R, S_prime, Done = memory.sample(batch_size)\n",
        "    # 2. 타겟 y_i 계산 (Target Network 사용) --> Moving target 문제를 해결하기 위해..\n",
        "    q_target_out = q_target(S_prime)\n",
        "    # 3. 현재 Q값 계산 (Q-Network 사용)\n",
        "    q_out = q(S)\n",
        "    # 4. Loss 계산 및 역전파(Backpropagation)\n",
        "    q_a = q_out.gather(1, A) # gather함수 --> 1-dim(열 방향)의 행동 A값 인덱스의 q값만 추출하는 코드\n",
        "    max_q_prime = q_target_out.max(1)[0].unsqueeze(1) # max(1)은 두가지 반환 --> [0]에서 최대값, [1]에서 argmax / unsqueeze를 통해 q_a와 차원을 맞춤([batchsize]1차원 --> [batchsize, 1]로)\n",
        "    target = R + gamma * max_q_prime * Done\n",
        "    loss = F.smooth_l1_loss(q_a, target) # L = 1/2 * (Target - current_Q)^2 의 식으로 계산\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward() # L의 gradients 계산\n",
        "    torch.nn.utils.clip_grad_norm_(q.parameters(), 1.0) # 안정장치 - norm이 1.0을 넘어가는 보상에 대해서 1.0으로 고\n",
        "    optimizer.step() # Q_net의 가중치 학습"
      ],
      "metadata": {
        "id": "6v_L0d5Pf9-d"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 메인 루프 (Main Loop)\n",
        "def main():\n",
        "    env = gym.make('CartPole-v1')\n",
        "    q = Qnet()\n",
        "    q_target = Qnet()\n",
        "    q_target.load_state_dict(q.state_dict()) # 가중치 동기화\n",
        "    memory = ReplayBuffer()\n",
        "\n",
        "    optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
        "    score = 0.0\n",
        "\n",
        "    for n_epi in range(10000):\n",
        "        epsilon = max(0.01, 0.3 - 0.3*(n_epi/2000)) # Linear annealing(선형 감쇠) --> 학습을 해 나갈수록 입실론 값을 줄여나감\n",
        "        s, _ = env.reset() # env.reset() --> 환경을 초기 상태로 reset + 첫번째 state을 반환\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # 에이전트가 행동을 선택하고 환경과 상호작용\n",
        "            action = q.sample_action(torch.from_numpy(s).float(), epsilon)\n",
        "            s_prime, r, done, _, _ = env.step(action)\n",
        "            memory.put((s, action, r/100.0, s_prime, done)) # 딥러닝은 보통 입출력이 -1~1 일때 최적화가 가장 잘 작동.. normalization을 통해 학습의 안정화를 꾀함\n",
        "            # 4. 학습 조건이 맞으면 train(q, q_target, memory, optimizer) 호출\n",
        "            if memory.size() > 2000:\n",
        "                train(q, q_target, memory, optimizer)\n",
        "\n",
        "            s = s_prime\n",
        "            score += r\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # 일정 주기마다 Target Network를 업데이트(가중치 복사)\n",
        "\n",
        "        if n_epi % 20 == 0 and n_epi != 0:\n",
        "            # 학습 중 평균 점수 (탐험 때문에 낮을 수 있음)\n",
        "            avg_score = score / 20\n",
        "\n",
        "            # 잠깐 입실론 0으로 설정하고 1판 테스트 (진짜 실력 측정)\n",
        "            test_score = 0\n",
        "            s_test, _ = env.reset()\n",
        "            done_test = False\n",
        "            while not done_test:\n",
        "                # 입실론 0.0 (무조건 Greedy)\n",
        "                a_test = q.sample_action(torch.from_numpy(s_test).float(), 0.0)\n",
        "                s_test, r_test, done_test, _, _ = env.step(a_test)\n",
        "                test_score += r_test\n",
        "\n",
        "            # 결과 출력\n",
        "            print(f\"Episode: {n_epi} | Train Score: {avg_score:.1f} | Real Test: {test_score:.1f} | Eps: {epsilon:.2f}\")\n",
        "\n",
        "            # 4. 모델 저장 및 타겟 업데이트\n",
        "            q_target.load_state_dict(q.state_dict())\n",
        "            score = 0.0\n",
        "\n",
        "    env.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "ow0AgkGAf-MB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0716a3f6-472c-46ff-b8d1-cfab0152940d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 20 | Train Score: 11.2 | Real Test: 10.0 | Eps: 0.30\n",
            "Episode: 40 | Train Score: 11.6 | Real Test: 9.0 | Eps: 0.29\n",
            "Episode: 60 | Train Score: 11.8 | Real Test: 10.0 | Eps: 0.29\n",
            "Episode: 80 | Train Score: 11.5 | Real Test: 10.0 | Eps: 0.29\n",
            "Episode: 100 | Train Score: 11.3 | Real Test: 10.0 | Eps: 0.28\n",
            "Episode: 120 | Train Score: 12.5 | Real Test: 9.0 | Eps: 0.28\n",
            "Episode: 140 | Train Score: 10.6 | Real Test: 10.0 | Eps: 0.28\n",
            "Episode: 160 | Train Score: 10.9 | Real Test: 10.0 | Eps: 0.28\n",
            "Episode: 180 | Train Score: 13.4 | Real Test: 20.0 | Eps: 0.27\n",
            "Episode: 200 | Train Score: 22.3 | Real Test: 88.0 | Eps: 0.27\n",
            "Episode: 220 | Train Score: 40.4 | Real Test: 50.0 | Eps: 0.27\n",
            "Episode: 240 | Train Score: 27.1 | Real Test: 36.0 | Eps: 0.26\n",
            "Episode: 260 | Train Score: 21.4 | Real Test: 12.0 | Eps: 0.26\n",
            "Episode: 280 | Train Score: 67.5 | Real Test: 205.0 | Eps: 0.26\n",
            "Episode: 300 | Train Score: 79.6 | Real Test: 294.0 | Eps: 0.26\n",
            "Episode: 320 | Train Score: 122.8 | Real Test: 203.0 | Eps: 0.25\n",
            "Episode: 340 | Train Score: 160.8 | Real Test: 126.0 | Eps: 0.25\n",
            "Episode: 360 | Train Score: 129.4 | Real Test: 140.0 | Eps: 0.25\n",
            "Episode: 380 | Train Score: 114.7 | Real Test: 119.0 | Eps: 0.24\n",
            "Episode: 400 | Train Score: 87.9 | Real Test: 108.0 | Eps: 0.24\n",
            "Episode: 420 | Train Score: 82.9 | Real Test: 104.0 | Eps: 0.24\n",
            "Episode: 440 | Train Score: 61.6 | Real Test: 99.0 | Eps: 0.23\n",
            "Episode: 460 | Train Score: 117.5 | Real Test: 99.0 | Eps: 0.23\n",
            "Episode: 480 | Train Score: 109.7 | Real Test: 125.0 | Eps: 0.23\n",
            "Episode: 500 | Train Score: 139.2 | Real Test: 114.0 | Eps: 0.22\n",
            "Episode: 520 | Train Score: 114.9 | Real Test: 117.0 | Eps: 0.22\n",
            "Episode: 540 | Train Score: 133.6 | Real Test: 235.0 | Eps: 0.22\n",
            "Episode: 560 | Train Score: 134.3 | Real Test: 115.0 | Eps: 0.22\n",
            "Episode: 580 | Train Score: 143.6 | Real Test: 254.0 | Eps: 0.21\n",
            "Episode: 600 | Train Score: 117.0 | Real Test: 125.0 | Eps: 0.21\n",
            "Episode: 620 | Train Score: 131.2 | Real Test: 180.0 | Eps: 0.21\n",
            "Episode: 640 | Train Score: 128.9 | Real Test: 126.0 | Eps: 0.20\n",
            "Episode: 660 | Train Score: 137.8 | Real Test: 181.0 | Eps: 0.20\n",
            "Episode: 680 | Train Score: 139.2 | Real Test: 859.0 | Eps: 0.20\n",
            "Episode: 700 | Train Score: 142.8 | Real Test: 159.0 | Eps: 0.20\n",
            "Episode: 720 | Train Score: 114.6 | Real Test: 112.0 | Eps: 0.19\n",
            "Episode: 740 | Train Score: 148.9 | Real Test: 101.0 | Eps: 0.19\n",
            "Episode: 760 | Train Score: 134.8 | Real Test: 232.0 | Eps: 0.19\n",
            "Episode: 780 | Train Score: 150.9 | Real Test: 376.0 | Eps: 0.18\n",
            "Episode: 800 | Train Score: 218.7 | Real Test: 587.0 | Eps: 0.18\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2731052726.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2731052726.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;31m# 입실론 0.0 (무조건 Greedy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0ma_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0ms_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mtest_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1599172828.py\u001b[0m in \u001b[0;36msample_action\u001b[0;34m(self, obs, epsilon)\u001b[0m\n\u001b[1;32m     20\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 신경망의 결과는 각 행동들의 Q함수 --> 따라서 argmax를 이용해 행동 자체를 뽑아냄\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1599172828.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m     \u001b[0;31m# fmt: off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1780\u001b[0;31m         \u001b[0mforward_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1781\u001b[0m         \u001b[0;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m         \u001b[0;31m# this function, and just call forward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}